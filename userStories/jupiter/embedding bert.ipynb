{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe7fe241",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vscode/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !pip install transformers torch torchvision torchaudio\n",
    "\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from scipy.spatial.distance import cosine\n",
    "import numpy as np\n",
    "\n",
    "# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n",
    "import logging\n",
    "#logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# Load pre-trained model (weights)\n",
    "model = BertModel.from_pretrained('bert-base-uncased',\n",
    "                                  output_hidden_states = True, # Whether the model returns all hidden-states.\n",
    "                                  )\n",
    "\n",
    "# Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db85b8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(text):\n",
    "  text = '[CLS] ' + text + ' [SEP]'\n",
    "  # Split the sentence into tokens.\n",
    "  tokenized_text = tokenizer.tokenize(text)\n",
    "\n",
    "  # Map the token strings to their vocabulary indeces.\n",
    "  indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "  segments_ids = [1] * len(tokenized_text)\n",
    "  # Convert inputs to PyTorch tensors\n",
    "  tokens_tensor = torch.tensor([indexed_tokens])\n",
    "  segments_tensors = torch.tensor([segments_ids])\n",
    "\n",
    "  # Run the text through BERT, and collect all of the hidden states produced\n",
    "  # from all 12 layers. \n",
    "  with torch.no_grad():\n",
    "\n",
    "      outputs = model(tokens_tensor, segments_tensors)\n",
    "\n",
    "      # Evaluating the model will return a different number of objects based on \n",
    "      # how it's  configured in the `from_pretrained` call earlier. In this case, \n",
    "      # becase we set `output_hidden_states = True`, the third item will be the \n",
    "      # hidden states from all layers. See the documentation for more details:\n",
    "      # https://huggingface.co/transformers/model_doc/bert.html#bertmodel\n",
    "      hidden_states = outputs[2]\n",
    "  \n",
    "  return (tokenized_text, hidden_states)\n",
    "\n",
    "def get_token_embeddings(hidden_states):\n",
    "  # Concatenate the tensors for all layers. We use `stack` here to\n",
    "  # create a new dimension in the tensor.\n",
    "  token_embeddings = torch.stack(hidden_states, dim=0)\n",
    "\n",
    "  # Remove dimension 1, the \"batches\".\n",
    "  token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "\n",
    "  # Swap dimensions 0 and 1.\n",
    "  token_embeddings = token_embeddings.permute(1,0,2)\n",
    "  # contains: Tokens, layers, features\n",
    "\n",
    "  return token_embeddings\n",
    "\n",
    "def vectorize_tokens(embeddings, method='cat'):\n",
    "   # Stores the token vectors, with shape [22 x 3,072] for cat or [22 x 768] for sum\n",
    "  token_vecs = []\n",
    "\n",
    "  # `embeddings` is a [22 x 12 x 768] tensor.\n",
    "\n",
    "  for token in embeddings:\n",
    "    # `token` is a [12 x 768] tensor\n",
    "\n",
    "    if method == 'cat':\n",
    "      # Concatenate the vectors (that is, append them together) from the last \n",
    "      # four layers.\n",
    "      # Each layer vector is 768 values, so `cat_vec` is length 3,072.\n",
    "      vec = torch.cat((token[-1], token[-2], token[-3], token[-4]), dim=0)\n",
    "    elif method == 'catMiddle':\n",
    "      vec = torch.cat((token[-2], token[-3], token[-4], token[-5], token[-6], token[-7], token[-8], token[-9], token[-10]), dim=0)\n",
    "    elif method == 'sum':\n",
    "      # Sum the vectors from the last four layers.\n",
    "      vec = torch.sum(token[-4:], dim=0)\n",
    "    else:\n",
    "      raise BaseException('Unkown method')\n",
    "\n",
    "    token_vecs.append(vec)\n",
    "  return token_vecs\n",
    "\n",
    "def vectorize_sentence(hidden_states):\n",
    "  # `hidden_states` has shape [13 x 1 x 22 x 768]\n",
    "\n",
    "  # `token_vecs` is a tensor with shape [22 x 768]\n",
    "  token_vecs = hidden_states[-2][0]\n",
    "\n",
    "  # Calculate the average of all 22 token vectors.\n",
    "  sentence_embedding = torch.mean(token_vecs, dim=0)\n",
    "  return sentence_embedding\n",
    "  # print (\"Our final sentence embedding vector of shape:\", sentence_embedding.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a7528b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'a', 'web', 'server', 'on', 'port', '3000', 'with', 'parameters', 'user', '##name', 'of', 'type', 'string', 'and', 'password', 'of', 'type', 'string', '[SEP]']\n",
      "['[CLS]', 'web', 'server', 'on', 'port', 'xx', '##xx', 'with', 'parameters', 'xx', '##xx', 'of', 'type', 'xx', '##xx', '[SEP]']\n",
      "0.7960519790649414\n",
      "0.8924217224121094\n"
     ]
    }
   ],
   "source": [
    "tokenized_text, hidden_states = get_embeddings(\"a web server on port 3000 with parameters username of type string and password of type string\")\n",
    "token_embeddings = get_token_embeddings(hidden_states)\n",
    "token_vec = vectorize_tokens(token_embeddings, method='cat')\n",
    "print(tokenized_text)\n",
    "\n",
    "test_tokenized_text, test_hidden_states = get_embeddings('web server on port xxxx with parameters xxxx of type xxxx')\n",
    "test = vectorize_tokens(get_token_embeddings(test_hidden_states), method='cat')\n",
    "print(test_tokenized_text)\n",
    "\n",
    "print(1 - cosine(token_vec[2], test[1]))\n",
    "print(1 - cosine(vectorize_sentence(hidden_states), vectorize_sentence(test_hidden_states)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff6b08d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]        [CLS]        0.9652470946311951\n",
      "a            a            0.9581173062324524\n",
      "http         web          0.7633567452430725\n",
      "server       server       0.9401279091835022\n",
      "on           on           0.9609048366546631\n",
      "port         port         0.9547920227050781\n",
      "3000         3000         0.9673320055007935\n",
      "with         with         0.9559084177017212\n",
      "parameters   parameters   0.9227150082588196\n",
      "email        user         0.6826009750366211\n",
      "of           of           0.8307011127471924\n",
      "type         type         0.8420049548149109\n",
      "text         string       0.6760694980621338\n",
      "[SEP]        and          -0.010823934338986874\n",
      "             password     \n",
      "             of           \n",
      "             type         \n",
      "             string       \n",
      "             [SEP]        \n"
     ]
    }
   ],
   "source": [
    "def find_similar_words(*sentences):\n",
    "  tokenizeds = []\n",
    "  token_vecs = []\n",
    "  for sentence in sentences:\n",
    "    tokenized, hidden_states = get_embeddings(sentence)\n",
    "    tokenizeds.append(tokenized)\n",
    "    token_vecs.append(vectorize_tokens(get_token_embeddings(hidden_states), method='cat'))\n",
    "\n",
    "  max_length = max(map(len, tokenizeds))\n",
    "  for i in range(max_length):\n",
    "    for sid in range(len(sentences)):\n",
    "      print('{:<12} '.format(tokenizeds[sid][i] if len(tokenizeds[sid]) > i else ''), end='')\n",
    "\n",
    "    if len(tokenizeds[0]) > i and len(tokenizeds[1]) > i:\n",
    "      print('{:<12}'.format(1 - cosine(token_vecs[0][i], token_vecs[1][i])))\n",
    "    else:\n",
    "      print()\n",
    "\n",
    "find_similar_words('a http server on port 3000 with parameters email of type text', 'a web server on port 3000 with parameters user of type string and password of type string')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e2a6811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.spatial.distance import cosine\n",
    "\n",
    "# # Calculate the cosine similarity between the word bank \n",
    "# # in \"bank robber\" vs \"river bank\" (different meanings).\n",
    "# diff_bank = 1 - cosine(token_vecs_sum[10], token_vecs_sum[19])\n",
    "\n",
    "# # Calculate the cosine similarity between the word bank\n",
    "# # in \"bank robber\" vs \"bank vault\" (same meaning).\n",
    "# same_bank = 1 - cosine(token_vecs_sum[10], token_vecs_sum[6])\n",
    "\n",
    "# print('Vector similarity for  *similar*  meanings:  %.2f' % same_bank)\n",
    "# print('Vector similarity for *different* meanings:  %.2f' % diff_bank)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
