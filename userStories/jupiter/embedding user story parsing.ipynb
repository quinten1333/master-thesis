{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe7fe241",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vscode/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-05-12 08:32:33.128508: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-05-12 08:32:33.133224: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-05-12 08:32:33.185122: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-05-12 08:32:33.187344: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-12 08:32:33.910206: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "#!pip install transformers tensorflow\n",
    "\n",
    "from transformers import BertTokenizer, TFBertModel, pipeline\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e29eced",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = TFBertModel.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0bae8103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bert (TFBertMainLayer)      multiple                  109482240 \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 109,482,240\n",
      "Trainable params: 109,482,240\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9a4079c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.240368\n"
     ]
    }
   ],
   "source": [
    "def embed(text):\n",
    "  encoded_input = tokenizer(\"[CLS]\" + text + \"[SEP]\", return_tensors='tf')\n",
    "  return model(encoded_input).last_hidden_state\n",
    "\n",
    "def similarity(a, b):\n",
    "  return np.sqrt(np.sum((a-b)**2))\n",
    "\n",
    "res = embed(\"a http server on port 3000 with parameters username of type string\").numpy()\n",
    "# res = embed(\"At the UvA we research microservices architectures for the sake of humanity\").numpy()\n",
    "http = embed('http server').numpy()\n",
    "print(similarity(res[0][2], http[0][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb984dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': <tf.Tensor: shape=(1, 5), dtype=int32, numpy=array([[ 101,  101, 2158,  102,  102]], dtype=int32)>, 'token_type_ids': <tf.Tensor: shape=(1, 5), dtype=int32, numpy=array([[0, 0, 0, 0, 0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 5), dtype=int32, numpy=array([[1, 1, 1, 1, 1]], dtype=int32)>}\n",
      "{'input_ids': <tf.Tensor: shape=(1, 5), dtype=int32, numpy=array([[ 101,  101, 2332,  102,  102]], dtype=int32)>, 'token_type_ids': <tf.Tensor: shape=(1, 5), dtype=int32, numpy=array([[0, 0, 0, 0, 0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 5), dtype=int32, numpy=array([[1, 1, 1, 1, 1]], dtype=int32)>}\n",
      "{'input_ids': <tf.Tensor: shape=(1, 5), dtype=int32, numpy=array([[ 101,  101, 2450,  102,  102]], dtype=int32)>, 'token_type_ids': <tf.Tensor: shape=(1, 5), dtype=int32, numpy=array([[0, 0, 0, 0, 0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 5), dtype=int32, numpy=array([[1, 1, 1, 1, 1]], dtype=int32)>}\n",
      "{'input_ids': <tf.Tensor: shape=(1, 5), dtype=int32, numpy=array([[ 101,  101, 3035,  102,  102]], dtype=int32)>, 'token_type_ids': <tf.Tensor: shape=(1, 5), dtype=int32, numpy=array([[0, 0, 0, 0, 0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 5), dtype=int32, numpy=array([[1, 1, 1, 1, 1]], dtype=int32)>}\n",
      "[[[-0.23893274  0.11770701  0.08827327 ... -0.14755237  0.00234456\n",
      "   -0.07584715]\n",
      "  [-0.27329203  0.15269382  0.12415472 ... -0.20440409  0.0940225\n",
      "   -0.04132102]\n",
      "  [-0.14138152 -0.88975024  0.09675613 ... -0.23879431  0.19173808\n",
      "   -0.7444078 ]\n",
      "  [ 0.95022875  0.16267301 -0.322513   ... -0.09814322 -0.98599404\n",
      "   -0.3643402 ]\n",
      "  [ 0.9446005   0.1628111  -0.31610304 ... -0.09956188 -0.9944441\n",
      "   -0.3661971 ]]]\n",
      "12.636351\n"
     ]
    }
   ],
   "source": [
    "man = embed(\"man\").numpy()\n",
    "king = embed(\"king\").numpy()\n",
    "woman = embed(\"woman\").numpy()\n",
    "queen = embed('queen').numpy()\n",
    "\n",
    "test = woman + (king - man)\n",
    "print(queen)\n",
    "\n",
    "print(np.sqrt(np.sum((queen-test)**2)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
